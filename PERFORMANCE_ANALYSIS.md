# An√°lisis de Rendimiento - Ë≥ÉÈáëÂè∞Â∏≥ Generator v4 PRO

## üìä Diagn√≥stico Completo

### 1. ‚ùå PROBLEMA CR√çTICO: Reprocesamiento de Archivos Excel

**S√≠ntoma:** La aplicaci√≥n se siente lenta porque est√° reprocesando archivos Excel completos en cada request.

**Evidencia en logs:**
```
[Procesando] hoja: totalChin
[INFO] Columnas encontradas: 53
[Procesando] hoja Ë´ãË≤† (formato vertical)...
[INFO] Encontrados 64 bloques de empleados Ë´ãË≤†Á§æÂì°
[INFO] Procesados 60 empleados Ë´ãË≤†Á§æÂì°
```

**Causa Ra√≠z:** M√∫ltiples llamadas API est√°n reprocesando archivos cuando NO deber√≠an:

1. **`/api/ukeoi-employees`** - Se llama frecuentemente y puede estar leyendo archivos
2. **`/api/employees-by-company/{company}`** - Consulta DB correctamente, pero puede estar llamando a procesador
3. Cada b√∫squeda de empleado parece triggerar procesamiento

**Impacto:**
- ‚è±Ô∏è **Tiempo de respuesta:** 3-8 segundos por request
- üíæ **Memoria:** Alto consumo procesando 60-70 empleados cada vez
- üîÑ **Carga CPU:** Procesamiento innecesario de Excel

---

### 2. üíæ Base de Datos Grande (25.9 MB)

**Datos actuales:**
- Tama√±o: 25,956,352 bytes (~26 MB)
- Sin √≠ndices optimizados para b√∫squedas frecuentes
- Modo WAL activado (correcto)

**Recomendaciones:**
- ‚úÖ Tama√±o aceptable para SQLite
- ‚ö†Ô∏è Verificar √≠ndices en:
  - `payroll_records.employee_id`
  - `payroll_records.period`
  - `haken_employees.dispatch_company`
  - `ukeoi_employees.job_type`

---

### 3. üìÅ Muchos Archivos Generados (1,164 archivos)

**Problema:**
- 1,164 archivos en carpeta `outputs/`
- No hay limpieza autom√°tica de archivos antiguos
- Posible impacto en rendimiento del filesystem

**Recomendaci√≥n:**
- Implementar limpieza autom√°tica de archivos >7 d√≠as
- Considerar mover a subcarpetas por a√±o/mes

---

### 4. üîÑ Requests Duplicados

**Evidencia:** M√∫ltiples requests al mismo endpoint en r√°pida sucesi√≥n:
```
GET /api/employee/250201
GET /api/employee/250201
GET /api/employee/250201 (repeated 4-5 times)
```

**Causa:** Posible problema en frontend:
- JavaScript puede estar haciendo llamadas duplicadas
- Falta debouncing en b√∫squedas
- No hay cach√© del lado del cliente

---

### 5. ‚ö†Ô∏è Sin Sistema de Cach√©

**Problema:**
- Cada request va directo a base de datos
- No hay cach√© en memoria para datos frecuentes
- Datos maestros (Âì°Âêç, Ê¥æÈÅ£ÂÖà) se cargan repetidamente

**Soluciones recomendadas:**
```python
from functools import lru_cache
from datetime import datetime, timedelta

# Cache simple para datos maestros
@lru_cache(maxsize=100, typed=False)
def get_employee_master_cached(employee_id: str):
    return get_employee_master(employee_id)

# Cache con TTL para listas
employee_list_cache = {
    'data': None,
    'timestamp': None,
    'ttl': 300  # 5 minutos
}
```

---

## üîç An√°lisis Detallado de Endpoints Lentos

### A. `/api/ukeoi-employees`
**Frecuencia:** Se llama cada vez que se carga la p√°gina
**Problema:** Puede estar reprocesando archivos en lugar de consultar DB
**Soluci√≥n:** Verificar que solo consulte base de datos

### B. `/api/employee/{id}/chingin-v2`
**Problema:** Endpoint funciona pero genera archivos cada vez
**Soluci√≥n:** Implementar cach√© de archivos generados (key: employee_id + year + format)

### C. `/api/employee/{id}/preview`
**Problema:** Consulta DB correctamente pero se llama m√∫ltiples veces
**Soluci√≥n:** A√±adir debouncing en frontend

---

## üöÄ Plan de Optimizaci√≥n Prioritario

### Fase 1: Fixes Cr√≠ticos (1-2 horas)

#### 1.1 Eliminar Reprocesamiento de Excel ‚ö° CR√çTICO
```python
# En app.py - verificar que estos endpoints NO llamen a processor.read_excel()
@app.get("/api/ukeoi-employees")
async def get_ukeoi_list():
    # DEBE llamar directamente a:
    return JSONResponse(get_all_ukeoi_employees())
    # NO debe llamar a processor.read_excel()
```

#### 1.2 A√±adir Debouncing en Frontend
```javascript
// En index.html - para b√∫squedas de empleado
let searchTimeout;
function searchEmployeeDebounced() {
    clearTimeout(searchTimeout);
    searchTimeout = setTimeout(() => {
        searchEmployee();
    }, 300); // 300ms delay
}
```

#### 1.3 A√±adir √çndices a Base de Datos
```sql
CREATE INDEX IF NOT EXISTS idx_payroll_emp_period
    ON payroll_records(employee_id, period);

CREATE INDEX IF NOT EXISTS idx_haken_dispatch
    ON haken_employees(dispatch_company)
    WHERE status IN ('Âú®ËÅ∑‰∏≠', 'ÁèæÂú®');

CREATE INDEX IF NOT EXISTS idx_ukeoi_jobtype
    ON ukeoi_employees(job_type)
    WHERE status IN ('Âú®ËÅ∑‰∏≠', 'ÁèæÂú®');
```

### Fase 2: Mejoras de Cach√© (2-3 horas)

#### 2.1 Cache Simple para Datos Maestros
```python
from functools import lru_cache
import time

# Cache con TTL manual
class SimpleCache:
    def __init__(self, ttl=300):
        self.cache = {}
        self.ttl = ttl

    def get(self, key):
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            else:
                del self.cache[key]
        return None

    def set(self, key, value):
        self.cache[key] = (value, time.time())

# Instanciar caches
employee_cache = SimpleCache(ttl=300)  # 5 minutos
company_cache = SimpleCache(ttl=600)   # 10 minutos
```

#### 2.2 Cache de Archivos Generados
```python
# En excel_processor.py
def generate_chingin_format_b(self, employee_id, year, output_path=None):
    # Verificar si ya existe archivo reciente
    cache_key = f"{employee_id}_{year}_b"
    cache_path = f"outputs/cache/chingin_{cache_key}.xlsx"

    if os.path.exists(cache_path):
        # Si archivo tiene menos de 1 hora, reutilizarlo
        file_age = time.time() - os.path.getmtime(cache_path)
        if file_age < 3600:  # 1 hora
            return {"status": "success", "file_path": cache_path, "cached": True}

    # Generar nuevo archivo...
```

### Fase 3: Limpieza y Mantenimiento (1 hora)

#### 3.1 Limpieza Autom√°tica de Archivos Antiguos
```python
# En app.py o como tarea programada
import os
import time

def cleanup_old_files(directory="outputs", days_old=7):
    """Eliminar archivos m√°s antiguos de X d√≠as"""
    cutoff_time = time.time() - (days_old * 86400)
    deleted_count = 0

    for root, dirs, files in os.walk(directory):
        for file in files:
            filepath = os.path.join(root, file)
            if os.path.getmtime(filepath) < cutoff_time:
                try:
                    os.remove(filepath)
                    deleted_count += 1
                except Exception as e:
                    print(f"Error eliminando {filepath}: {e}")

    return deleted_count

# Ejecutar al inicio de la app
@app.on_event("startup")
async def startup_event():
    cleanup_old_files(days_old=7)
```

---

## üìà Mejoras Esperadas

### Antes de Optimizaci√≥n:
- ‚è±Ô∏è Tiempo de carga inicial: 3-5 segundos
- üîÑ Tiempo de b√∫squeda empleado: 2-4 segundos
- üì• Generaci√≥n Ë≥ÉÈáëÂè∞Â∏≥: 5-10 segundos
- üíæ Uso memoria: Alto (reprocesamiento constante)

### Despu√©s de Optimizaci√≥n (Estimado):
- ‚è±Ô∏è Tiempo de carga inicial: 0.5-1 segundo (80% mejora)
- üîÑ Tiempo de b√∫squeda empleado: 0.2-0.5 segundos (90% mejora)
- üì• Generaci√≥n Ë≥ÉÈáëÂè∞Â∏≥: 1-2 segundos primera vez, <0.5s con cach√© (95% mejora)
- üíæ Uso memoria: Bajo y predecible

---

## üîß Comandos de Diagn√≥stico

### Verificar tama√±o de DB:
```bash
powershell -Command "Get-Item chingin_data.db | Select-Object Length, Name"
```

### Contar archivos generados:
```bash
powershell -Command "(Get-ChildItem outputs -File -Recurse | Measure-Object).Count"
```

### Ver √≠ndices actuales:
```bash
sqlite3 chingin_data.db "SELECT name, sql FROM sqlite_master WHERE type='index';"
```

### Analizar queries lentas (agregar logging):
```python
import time
def log_query_time(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start
        if duration > 0.5:  # Log si toma >500ms
            print(f"‚ö†Ô∏è SLOW QUERY: {func.__name__} took {duration:.2f}s")
        return result
    return wrapper
```

---

## ‚úÖ Checklist de Implementaci√≥n

### Inmediato (Hoy):
- [ ] Verificar que `/api/ukeoi-employees` no reprocese Excel
- [ ] A√±adir logging de timing en endpoints cr√≠ticos
- [ ] Implementar debouncing en b√∫squedas frontend
- [ ] A√±adir √≠ndices a base de datos

### Corto Plazo (Esta Semana):
- [ ] Implementar cach√© simple para datos maestros
- [ ] Cache de archivos Excel generados (1 hora TTL)
- [ ] Limpieza autom√°tica de archivos antiguos
- [ ] Optimizar queries frecuentes

### Medio Plazo (Este Mes):
- [ ] Considerar Redis para cach√© distribuido
- [ ] Implementar paginaci√≥n en listas grandes
- [ ] A√±adir compresi√≥n de responses
- [ ] Monitoring de performance con m√©tricas

---

## üìù Notas Adicionales

### Consideraciones:
1. **SQLite es adecuado** para este volumen de datos
2. **Modo WAL ya est√° activo** (correcto para concurrencia)
3. **No hay memory leaks evidentes** en logs
4. **Problema principal es reprocesamiento de Excel**

### Alternativas a Largo Plazo:
- Migrar archivos generados a S3/Azure Blob
- Implementar worker queue (Celery) para generaciones pesadas
- Considerar PostgreSQL si concurrencia aumenta significativamente
- Implementar WebSocket para actualizaciones en tiempo real

---

**Fecha de An√°lisis:** 2025-11-26
**Versi√≥n del Sistema:** v4 PRO
**Analizado por:** Claude Code Assistant
